{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "%matplotlib inline\n",
    "sns.set_style('darkgrid') # set the grid style for the seaborn plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.float_format', lambda x: '%.5f' % x) # suppress scientific notation in pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('EPL Player Stats All Time.xlsx', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.set_index('Player_ID', inplace=True) # set column 'Player_ID' as index w/o creating a duplicate\n",
    "df.index = df['Player_ID']\n",
    "df.index.names = ['ID']\n",
    "#df.dropna(inplace=True)\n",
    "df.fillna(0, inplace=True)\n",
    "#df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = df['Position'].reset_index(drop='True').tolist()\n",
    "a = set(a)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Yellow Cards'] = -1*df['Yellow Cards']\n",
    "df['Red Cards'] = -1*df['Red Cards']\n",
    "df['Fouls Committed'] = -1*df['Fouls Committed']\n",
    "df['Turnovers'] = -1*df['Turnovers']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FILTERS\n",
    "\n",
    "df = df[df['90s Played'] >= 6] # drop players who have no appearances\n",
    "#df = df[df['Position'] == 'GK'].reset_index(drop='True') # Filter by a Single Position\n",
    "df = df[(df['Position'] == 'DF') | (df['Position'] == 'DFMF') | (df['Position'] == 'DFFW')].reset_index(drop='True') # Filter by Multiple Positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.columns.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PARAMETERS - Select 13 and include PLAYER and TEAM ----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "#Goalkeepers\n",
    "\n",
    "#df = df[['Player_ID','GK Saves','GK Save %','GK Post-Shot xG Against per Shot on Target','GK Post-Shot xG Against PlusMinus', 'GK Long Passes Completed', 'GK Long Pass Preference %', 'GK Average Pass Length (Yd)', 'GK Long Goal Kick Preference %','GK Average Goal Kick Length (Yd)', 'GK Sweeping Actions', 'GK Average Sweeping Action Distance From Goal (Yd)']]\n",
    "\n",
    "#Centerbacks\n",
    "\n",
    "df = df[['Player_ID','Pass Completion %', 'Pressures','Fouls Committed','Tackle Success %','Tackles Won','Interceptions','Aerials Won','Aerial Win %','Short Passes Completed','Long Passes Completed','Goal Contributions per 90']]\n",
    "\n",
    "#Fullbacks\n",
    "\n",
    "#df = df[['Player_ID','Tackles + Interceptions','Pressures', 'Deep Progressions','Share of Touches in the Final Third %','Pass Completion %','Goal Contributions per 90','Dribbles Completed', 'Turnovers', 'Aerials Won', 'Crosses Completed into Opposition Box', 'Tackle Success %']]\n",
    "\n",
    "#Midfielders\n",
    "\n",
    "#df = df[['Player_ID', 'Passes Completed','Pass Completion %','Deep Progressions','xA', 'Goal Contributions per 90', 'Dribbles Completed','Fouls Won','Turnovers','Successful Pressures', 'Pressures', 'Tackles + Interceptions']]\n",
    "\n",
    "#Attacking Midfielders/Wingers\n",
    "\n",
    "#df = df[['Player_ID', 'Non-Penalty xG','Shots', 'Touches In Opposition Box', 'Pass Completion %', 'Crosses Completed into Opposition Box', 'xA','Fouls Won','Dribbles Completed','Turnovers', 'Successful Pressures', 'Goals per Shot']]\n",
    "\n",
    "#Strikers\n",
    "\n",
    "#df = df[['Player_ID','Non-Penalty xG','Goals - xG','Shots','Touches In Opposition Box','Shots per Touch', 'xA','Successful Pressures','Pressures','Aerials Won','Dribbles Completed','Non-Penalty xG per Shot']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Principal Component Analysis (PCA)\n",
    "\n",
    "#df.describe().loc['std']\n",
    "#It seems that there's a better way to perform cluster analysis, that is to keep all the metrics that we want to assess but to give each metric some weight (that is, a level of importance) based on how much variation it accounts for. This is exactly what Principal Component Analysis (PCA) does.\n",
    "\n",
    "#PCA is a dimensionality reduction technique. It creates new features/metrics using linear combinations of the original features/metrics, that is, it gives a weight to each of the original metrics based on how much variation they explain. It combines these weighted metrics and forms a new feature. In this way, PCA keeps takes into account all the initial metrics but it also gives certain weightage to each of those.\n",
    "\n",
    "#These new features form the axes of the PCA plot and are known as Principal Components (PCs). In most of the cases, the first two PCs account for the vast majority of the variation that is observed in the data. Therefore, PC1 and PC2 can provide a really good approximation of the variation in the data for our cluster analysis. Also, the variation observed on the PC1 axis is more significant than the variation observed on the PC2 axis. Visualising the variation in the data also becomes very easy as PCA can reduce the dimensionality to two or three dimensions, without any significant loss of variation.\n",
    "\n",
    "#dft = df[['Player_ID','Non-Penalty xG', 'xA','Dribbles Completed']] az column isteniyorsa\n",
    "dft = df\n",
    "df1 = dft.drop(['Player_ID'], axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature scaling\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(df1) # compute the mean and standard deviation to be used for later scaling\n",
    "#StandardScaler(copy=True, with_mean=True, with_std=True)\n",
    "scaled_features = scaler.transform(df1)\n",
    "#scaled_data = pd.DataFrame(data=scaled_features, columns=df1.columns)\n",
    "#scaled_data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Implementing PCA\n",
    "\n",
    "#For our second cluster analysis, we will find and use the first two PCs. In most cases, PC1 and PC2 account for the vast majority of the variation in the data (we will find out the exact proportion of variation that each of the two PCs account for in our case).\n",
    "\n",
    "#We will also use the loading scores (loading scores are the weights assigned to each of the initial metrics) of PC1 to see which of the initial metrics explain a lot of the variation in the data. The metrics with higher loading scores or weights are the ones which help differentiate and cluster players better.\n",
    "\n",
    "#Furthermore, we will use K means clustering on these new features produced by PCA to cluster the central midfielders. We will also use the 'elbow method' to find a suitable value for the number of clusters (K).\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(scaled_features) # Now, we will find the PCs using the fit method \n",
    "pca_data = pca.transform(scaled_features) # Let's apply rotation and dimensionality reduction to transform the data to its first two PCs.\n",
    "\n",
    "sns.scatterplot(x=pca_data[:, 0], y=pca_data[:, 1])\n",
    "plt.xlabel('Principal component 1')\n",
    "plt.ylabel('Principal component 2')\n",
    "plt.show() #Earlier, it was difficult to visualise our data since had multiple dimensions (more than three). PCA helped us bring this high dimensional data to the two-dimensional space. Let's plot our data!\n",
    "\n",
    "#Variation observed along the PC1 axis is more significant than the variation observed along the PC2 axis. We will plot this data again after we perform clustering on this PCA data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Interpreting the components\n",
    "#Let's have a look at the metrics which were the most important in affecting which cluster a player was assigned to.\n",
    "\n",
    "pc1_loading_scores = pd.DataFrame(data=pca.components_[0], index=df1.columns, columns=['PC1 Loading Score'])\n",
    "pc1_loading_scores.sort_values(by='PC1 Loading Score', ascending=False, inplace=True)\n",
    "pc1_loading_scores\n",
    "\n",
    "# Negative loading scores indicate that the metric does not explain much variation in the date. \n",
    "# Like all machine learning and data analysis, clustering is dependent on the needs of your team. Communication of goals needs to occur or else clustering can only maintain a generic, topline level of usefulness rather than being specific to the needs of the club."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc2_loading_scores = pd.DataFrame(data=pca.components_[1], index=df1.columns, columns=['PC2 Loading Score'])\n",
    "pc2_loading_scores.sort_values(by='PC2 Loading Score', ascending=False, inplace=True)\n",
    "pc2_loading_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KMeans clustering \n",
    "\n",
    "# Let's first create a dataframe for the PCA data. - higher values indicate better performance according to positional metrics\n",
    "pca_df = pd.DataFrame(data=pca_data, index=dft['Player_ID'], columns=['PC 1', 'PC 2'])\n",
    "pca_df.reset_index(inplace=True) # reset the index\n",
    "pca_df.head(3)\n",
    "\n",
    "#We know that our clusters are significantly accurate when the variation within each cluster is as little as possible. Each time we add a new cluster, the total variation within each cluster is smaller than before. And when there is only one point per cluster, the variation is 0. The inertia attribute of a K means clustering model represents the sum of squared distances of samples to their closest cluster center. Sum of squared distances is a measure of variation. If we plot the inertia per value of K, we will notice that after a certain K value, the inertia doesn't decrease as quickly as it did before. This K value is called the 'elbow point' sometimes, and it is a suitable value for K in our K means clustering analysis.Let's apply the elbow method and find a suitable K value for our cluster analysis!\n",
    "\n",
    "#First of all, we will scale the PCA data.\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(pca_df.drop('Player_ID', axis=1)) # compute the mean and standard deviation to be used for later scaling\n",
    "scaled_pca_features = scaler.transform(pca_df.drop('Player_ID', axis=1)) # perform standardisation by centering and scaling\n",
    "scaled_pca_data = pd.DataFrame(data=scaled_pca_features, columns=pca_df.drop('Player_ID', axis=1).columns) # store the scaled features in a datafram\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Elbow method\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(pca_df.drop('Player_ID', axis=1)) # compute the mean and standard deviation to be used for later scaling\n",
    "scaled_pca_features = scaler.transform(pca_df.drop('Player_ID', axis=1)) # perform standardisation by centering and scaling\n",
    "scaled_pca_data = pd.DataFrame(data=scaled_pca_features, columns=pca_df.drop('Player_ID', axis=1).columns) # store the scaled features in a datafram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prior to applying the elbow method, we have to decide a range of possible number of clusters. For our cluster analysis, let's take a range of 1 to 10 and see the results.\n",
    "\n",
    "inertia = []\n",
    "for k in range(1, 11):\n",
    "    kmeans = KMeans(n_clusters=k)\n",
    "    kmeans.fit(X=scaled_pca_data)\n",
    "    inertia.append(kmeans.inertia_)\n",
    "elbow_df = pd.DataFrame({'Number of clusters':range(1, 11), 'Inertia':inertia})\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(elbow_df['Number of clusters'], elbow_df['Inertia'], marker='o')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.xticks(np.arange(1, len(inertia)+1, 1))\n",
    "plt.ylabel('Inertia')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KMeans cluster creation\n",
    "\n",
    "kmeans = KMeans(n_clusters=3)\n",
    "kmeans.fit(X=scaled_pca_data)\n",
    "pca_df['Cluster Label'] = kmeans.labels_ # add cluster labels for players as a new column to the pca_df dataframe\n",
    "print(pca_df.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis and visualisation\n",
    "\n",
    "sns.scatterplot(x='PC 1', y='PC 2', data=pca_df, hue='Cluster Label', palette='viridis')\n",
    "plt.xlabel('Principal component 1')\n",
    "plt.ylabel('Principal component 2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, let's check out all the three clusters and the players they include.\n",
    "pca_cluster_0 = pca_df[pca_df['Cluster Label']==0]['Player_ID']\n",
    "pca_cluster_1 = pca_df[pca_df['Cluster Label']==1]['Player_ID']\n",
    "pca_cluster_2 = pca_df[pca_df['Cluster Label']==2]['Player_ID']\n",
    "#pca_cluster_3 = pca_df[pca_df['Cluster Label']==3]['Player_ID']\n",
    "#pca_cluster_4 = pca_df[pca_df['Cluster Label']==4]['Player_ID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\nCluster 0 players:\\n')\n",
    "print(pca_cluster_0)\n",
    "print('\\n\\n')\n",
    "print('Cluster 1 players:\\n')\n",
    "print(pca_cluster_1)\n",
    "print('\\n\\n')\n",
    "print('Cluster 2 players:\\n')\n",
    "print(pca_cluster_2)\n",
    "#print('\\n\\n')\n",
    "#print('Cluster 3 players:\\n')\n",
    "#print(pca_cluster_3)\n",
    "#print('\\n\\n')\n",
    "#print('Cluster 4 players:\\n')\n",
    "#print(pca_cluster_4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The only thing left is to prepare one final dataset/dataframe that we'll give to the team staff concerned with recruitment. This final dataset will be helpful to them during the recruitment process and all other relevant activities. We will keep only those metrics in this final dataset that the front office staff wants to see (in our case, the metrics that I want to see)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data = pd.merge(left=pca_df[['Player_ID','PC 1','PC 2', 'Cluster Label']],\n",
    "                      right=df,\n",
    "                      how='inner', on='Player_ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data = final_data.sort_values(by='PC 1', ascending=False)\n",
    "#final_data.sort_values(by='PC 1', ascending=False).head(10)\n",
    "final_data.reset_index(drop=True, inplace=True)\n",
    "final_data"
   ]
  }
 ]
}