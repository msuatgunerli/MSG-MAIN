cls
clear
clc
exit()
print(table)
install.packages("RSelenium")
print(table)
install.packages("RSelenium")
remDr <- RSelenium::remoteDriver(remoteServerAddr = "localhost",port = 4445L,browserName = "chrome")
remDr$open()
remDr$navigate("https://fbref.com/en/comps/22/passing/Major-League-Soccer-Stats")
remDr$screenshot(display = TRUE)
require("dplyr")
stats_passing_squads = xml2::read_html(remDr$getPageSource()[[1]]) %>% rvest::html_nodes("#stats_passing_squads") %>% rvest::html_table()
stats_passing_squads = stats_passing_squads[[1]]
head(bundesliga_players_fbref_shooting)
getFBrefStats = function(url,id){
require(RSelenium)
require(dplyr)
# For some unspecified reason we are starting and stopping the docker container initailly.
# Similar to heating the bike's engine before shifting the gears.
system("docker run -d -p 4445:4444 selenium/standalone-chrome")
t = system("docker ps",intern=TRUE)
if(is.na(as.character(strsplit(t[2],split = " ")[[1]][1]))==FALSE)
{
system(paste("docker stop ",as.character(strsplit(t[2],split = " ")[[1]][1]),sep=""))
}
# To avoid starting docker in Terminal
system("docker run -d -p 4445:4444 selenium/standalone-chrome")
Sys.sleep(3)
remDr <- RSelenium::remoteDriver(remoteServerAddr = "localhost", port = 4445L, browserName = "chrome")
# Automating the scraping initiation considering that Page navigation might crash sometimes in
# R Selenuium and we have to start the process again. Good to see that this while() logic
# works perfectly
while (TRUE) {
tryCatch({
#Entering our URL gets the browser to navigate to the page
remDr$open()
remDr$navigate(as.character(url))
}, error = function(e) {
remDr$close()
Sys.sleep(2)
print("slept 2 seconds")
next
}, finally = {
#remDr$screenshot(display = TRUE) #This will take a screenshot and display it in the RStudio viewer
break
})
}
# Scraping required stats
data <- xml2::read_html(remDr$getPageSource()[[1]]) %>%
rvest::html_nodes(id) %>%
rvest::html_table()
data = data[[1]]
remDr$close()
remove(remDr)
# Automating the following steps:
# 1. run "docker ps" in Terminal and get the container ID from the output
# 2. now run "docker stop container_id" e.g. docker stop f59930f56e38
t = system("docker ps",intern=TRUE)
system(paste("docker stop ",as.character(strsplit(t[2],split = " ")[[1]][1]),sep=""))
return(data)
}
bundesliga_players_fbref_shooting = getFBrefStats("https://fbref.com/en/comps/20/shooting/Bundesliga-Stats","#stats_shooting")
head(bundesliga_players_fbref_shooting)
print(table)
clear
require(rvest)
require(dplyr)
page <- read_html("http://acciotables.herokuapp.com/?page_url=https://fbref.com/en/comps/22/Major-League-Soccer-Stats&content_selector_id=%23stats_keeper_squads")
table <- page %>% html_table()
table <- table[[1]]
print(table)
source('D:/Desktop/Programming/Football Statistics/R/acciotables web scrapper/new method/aws_new_method.R', echo=TRUE)
require(rvest)
require(dplyr)
page <- read_html("http://acciotables.herokuapp.com/?page_url=https:/https://fbref.com/en/comps/26/Super-Lig-Stats&content_selector_id=%23#div_stats_standard_squads")
table <- page %>% html_table()
table <- table[[1]]
print(table)
# instructions ------------------------------------------------------------
#find the css selector id for each table and add after the content_selector_id= section, remember to swap # with the ASCII value %23 in the provided link
#the result is a table in the html format
require(rvest)
require(dplyr)
page <- read_html("http://acciotables.herokuapp.com/?page_url=https:/https://fbref.com/en/comps/26/Super-Lig-Stats&content_selector_id=%23#div_stats_standard_squads")
table <- page %>% html_table()
table <- table[[1]]
print(table)
# instructions ------------------------------------------------------------
#find the css selector id for each table and add after the content_selector_id= section, remember to swap # with the ASCII value %23 in the provided link
#the result is a table in the html format
require(rvest)
require(dplyr)
page <- read_html("http://acciotables.herokuapp.com/?page_url=https:/https://fbref.com/en/comps/26/Super-Lig-Stats&content_selector_id=%23#div_stats_standard_squads")
table <- page %>% html_table()
table <- table[[1]]
print(table)
# instructions ------------------------------------------------------------
#find the css selector id for each table and add after the content_selector_id= section, remember to swap # with the ASCII value %23 in the provided link
#the result is a table in the html format
require(rvest)
require(dplyr)
page <- read_html("http://acciotables.herokuapp.com/?page_url=https:/https://fbref.com/en/comps/26/Super-Lig-Stats&content_selector_id=%23#div_stats_standard_squads")
table <- page %>% html_table()
table <- table[[1]]
print(table)
# instructions ------------------------------------------------------------
#find the css selector id for each table and add after the content_selector_id= section, remember to swap # with the ASCII value %23 in the provided link
#the result is a table in the html format
require(rvest)
require(dplyr)
page <- read_html("http://acciotables.herokuapp.com/?page_url=https:/https://fbref.com/en/comps/26/Super-Lig-Stats&content_selector_id=%23#div_stats_standard_squads")
table <- page %>% html_table()
table <- table[[1]]
print(table)
# instructions ------------------------------------------------------------
#find the css selector id for each table and add after the content_selector_id= section, remember to swap # with the ASCII value %23 in the provided link
#the result is a table in the html format
page <- read_html("http://acciotables.herokuapp.com/?page_url=https:/https://fbref.com/en/comps/26/Super-Lig-Stats&content_selector_id=%23#div_stats_standard_squads")
table <- page %>% html_table()
table <- table[[1]]
print(table)
require(rvest)
require(dplyr)
page <- read_html("http://acciotables.herokuapp.com/?page_url=https:/https://fbref.com/en/comps/26/Super-Lig-Stats&content_selector_id=%23div_stats_standard_squads")
table <- page %>% html_table()
table <- table[[1]]
print(table)
# instructions ------------------------------------------------------------
#find the css selector id for each table and add after the content_selector_id= section, remember to swap # with the ASCII value %23 in the provided link
#the result is a table in the html format
page <- read_html("http://acciotables.herokuapp.com/?page_url=https:/https://fbref.com/en/comps/26/Super-Lig-Stats&content_selector_id=%23div_stats_standard_squads")
table <- page %>% html_table()
table <- table[[1]]
print(table)
page <- read_html("http://acciotables.herokuapp.com/?page_url=https:/https://fbref.com/en/comps/26/Super-Lig-Stats&content_selector_id=%23stats_standard_squads")
table <- page %>% html_table()
table <- table[[1]]
print(table)
require(rvest)
require(dplyr)
page <- read_html("http://acciotables.herokuapp.com/?page_url=https:/https://fbref.com/en/comps/26/Super-Lig-Stats&content_selector_id=%23#stats_keeper_squads")
table <- page %>% html_table()
table <- table[[1]]
print(table)
table <- table[[1]]
print(table)
page <- read_html("http://acciotables.herokuapp.com/?page_url=https:/https://fbref.com/en/comps/26/Super-Lig-Stats&content_selector_id=%23#stats_keeper_squads")
table <- page %>% html_table()
table <- table[[1]]
print(table)
page <- read_html("http://acciotables.herokuapp.com/?page_url=https://fbref.com/en/comps/26/Super-Lig-Stats&content_selector_id=%23#stats_keeper_squads")
table <- page %>% html_table()
table <- table[[1]]
print(table)
clear
page <- read_html("http://acciotables.herokuapp.com/?page_url=https://fbref.com/en/comps/26/Super-Lig-Stats&content_selector_id=%23stats_keeper_squads")
table <- page %>% html_table()
table <- table[[1]]
print(table)
page <- read_html("http://acciotables.herokuapp.com/?page_url=https://fbref.com/en/comps/26/Super-Lig-Stats&content_selector_id=%23stats_keeper_squads")
table <- page %>% html_table()
table <- table[[1]]
print(table)
page <- read_html("http://acciotables.herokuapp.com/?page_url=https://fbref.com/en/comps/26/Super-Lig-Stats&content_selector_id=%23stats_keeper_squads")
require(rvest)
require(dplyr)
page <- read_html("http://acciotables.herokuapp.com/?page_url=https://fbref.com/en/comps/26/Super-Lig-Stats&content_selector_id=%23stats_keeper_squads")
table <- page %>% html_table()
table <- table[[1]]
print(table)
require(rvest)
require(dplyr)
page <- read_html("http://acciotables.herokuapp.com/?page_url=https://fbref.com/en/comps/22/Major-League-Soccer-Stats&content_selector_id=%23stats_keeper_squads")
table <- page %>% html_table()
table <- table[[1]]
print(table)
# instructions ------------------------------------------------------------
#find the css selector id for each table and add after the content_selector_id= section, remember to swap # with the ASCII value %23 in the provided link
#the result is a table in the html format
table <- page %>% html_table()
table <- table[[1]]
print(table)
page <- read_html("http://acciotables.herokuapp.com/?page_url=https://fbref.com/en/comps/22/Major-League-Soccer-Stats&content_selector_id=%23stats_keeper_squads")
table <- page %>% html_table()
table <- table[[1]]
print(table)
require(rvest)
require(dplyr)
page <- read_html("http://acciotables.herokuapp.com/?page_url=https://fbref.com/en/comps/22/Major-League-Soccer-Stats&content_selector_id=%23stats_keeper_squads")
getFBrefStats = function(url,id){
require(RSelenium)
require(dplyr)
# For some unspecified reason we are starting and stopping the docker container initailly.
# Similar to heating the bike's engine before shifting the gears.
system("docker run -d -p 4445:4444 selenium/standalone-chrome")
t = system("docker ps",intern=TRUE)
if(is.na(as.character(strsplit(t[2],split = " ")[[1]][1]))==FALSE)
{
system(paste("docker stop ",as.character(strsplit(t[2],split = " ")[[1]][1]),sep=""))
}
# To avoid starting docker in Terminal
system("docker run -d -p 4445:4444 selenium/standalone-chrome")
Sys.sleep(3)
remDr <- RSelenium::remoteDriver(remoteServerAddr = "localhost", port = 4445L, browserName = "chrome")
# Automating the scraping initiation considering that Page navigation might crash sometimes in
# R Selenuium and we have to start the process again. Good to see that this while() logic
# works perfectly
while (TRUE) {
tryCatch({
#Entering our URL gets the browser to navigate to the page
remDr$open()
remDr$navigate(as.character(url))
}, error = function(e) {
remDr$close()
Sys.sleep(2)
print("slept 2 seconds")
next
}, finally = {
#remDr$screenshot(display = TRUE) #This will take a screenshot and display it in the RStudio viewer
break
})
}
# Scraping required stats
data <- xml2::read_html(remDr$getPageSource()[[1]]) %>%
rvest::html_nodes(id) %>%
rvest::html_table()
data = data[[1]]
remDr$close()
remove(remDr)
# Automating the following steps:
# 1. run "docker ps" in Terminal and get the container ID from the output
# 2. now run "docker stop container_id" e.g. docker stop f59930f56e38
t = system("docker ps",intern=TRUE)
system(paste("docker stop ",as.character(strsplit(t[2],split = " ")[[1]][1]),sep=""))
return(data)
}
# Target Website ----------------------------------------------------------
bundesliga_players_fbref_shooting =
getFBrefStats("https://fbref.com/en/comps/20/shooting/Bundesliga-Stats","#stats_shooting")
head(bundesliga_players_fbref_shooting)
getFBrefStats = function(url,id){
require(RSelenium)
require(dplyr)
# For some unspecified reason we are starting and stopping the docker container initailly.
# Similar to heating the bike's engine before shifting the gears.
system("docker run -d -p 4445:4444 selenium/standalone-chrome")
t = system("docker ps",intern=TRUE)
if(is.na(as.character(strsplit(t[2],split = " ")[[1]][1]))==FALSE)
{
system(paste("docker stop ",as.character(strsplit(t[2],split = " ")[[1]][1]),sep=""))
}
# To avoid starting docker in Terminal
system("docker run -d -p 4445:4444 selenium/standalone-chrome")
Sys.sleep(3)
remDr <- RSelenium::remoteDriver(remoteServerAddr = "localhost", port = 4445L, browserName = "chrome")
# Automating the scraping initiation considering that Page navigation might crash sometimes in
# R Selenuium and we have to start the process again. Good to see that this while() logic
# works perfectly
while (TRUE) {
tryCatch({
#Entering our URL gets the browser to navigate to the page
remDr$open()
remDr$navigate(as.character(url))
}, error = function(e) {
remDr$close()
Sys.sleep(2)
print("slept 2 seconds")
next
}, finally = {
#remDr$screenshot(display = TRUE) #This will take a screenshot and display it in the RStudio viewer
break
})
}
# Scraping required stats
data <- xml2::read_html(remDr$getPageSource()[[1]]) %>%
rvest::html_nodes(id) %>%
rvest::html_table()
data = data[[1]]
remDr$close()
remove(remDr)
# Automating the following steps:
# 1. run "docker ps" in Terminal and get the container ID from the output
# 2. now run "docker stop container_id" e.g. docker stop f59930f56e38
t = system("docker ps",intern=TRUE)
system(paste("docker stop ",as.character(strsplit(t[2],split = " ")[[1]][1]),sep=""))
return(data)
}
# Target Website ----------------------------------------------------------
bundesliga_players_fbref_shooting =
getFBrefStats("https://fbref.com/en/comps/20/shooting/Bundesliga-Stats","#stats_shooting")
head(bundesliga_players_fbref_shooting)
clear
cls
bundesliga_players_fbref_shooting =
getFBrefStats("https://fbref.com/en/comps/22/Major-League-Soccer-Stats","#stats_keeper_squads")
getFBrefStats = function(url,id){
require(RSelenium)
require(dplyr)
# For some unspecified reason we are starting and stopping the docker container initailly.
# Similar to heating the bike's engine before shifting the gears.
system("docker run -d -p 4445:4444 selenium/standalone-chrome")
t = system("docker ps",intern=TRUE)
if(is.na(as.character(strsplit(t[2],split = " ")[[1]][1]))==FALSE)
{
system(paste("docker stop ",as.character(strsplit(t[2],split = " ")[[1]][1]),sep=""))
}
# To avoid starting docker in Terminal
system("docker run -d -p 4445:4444 selenium/standalone-chrome")
Sys.sleep(3)
remDr <- RSelenium::remoteDriver(remoteServerAddr = "localhost", port = 4445L, browserName = "chrome")
# Automating the scraping initiation considering that Page navigation might crash sometimes in
# R Selenuium and we have to start the process again. Good to see that this while() logic
# works perfectly
while (TRUE) {
tryCatch({
#Entering our URL gets the browser to navigate to the page
remDr$open()
remDr$navigate(as.character(url))
}, error = function(e) {
remDr$close()
Sys.sleep(2)
print("slept 2 seconds")
next
}, finally = {
#remDr$screenshot(display = TRUE) #This will take a screenshot and display it in the RStudio viewer
break
})
}
# Scraping required stats
data <- xml2::read_html(remDr$getPageSource()[[1]]) %>%
rvest::html_nodes(id) %>%
rvest::html_table()
data = data[[1]]
remDr$close()
remove(remDr)
# Automating the following steps:
# 1. run "docker ps" in Terminal and get the container ID from the output
# 2. now run "docker stop container_id" e.g. docker stop f59930f56e38
t = system("docker ps",intern=TRUE)
system(paste("docker stop ",as.character(strsplit(t[2],split = " ")[[1]][1]),sep=""))
return(data)
}
# Target Website ----------------------------------------------------------
bundesliga_players_fbref_shooting =
getFBrefStats("https://fbref.com/en/comps/22/Major-League-Soccer-Stats","#stats_keeper_squads")
head(bundesliga_players_fbref_shooting)
site = "https://fbref.com/en/comps/22/Major-League-Soccer-Stats"
table = "#stats_keeper_squads"
target = paste("site", "," , "table")
print(target)
target = paste(site, "," , table)
print(target)
target = paste("""" , site, "," , table)
target = paste(" "" " , site, "," , table)
Table1 =
getFBrefStats("https://fbref.com/en/comps/22/Major-League-Soccer-Stats","#stats_keeper_squads")
head(Table1)
print(target)
head(Table1)
print(target)
table_name = "#stats_keeper_squads"
Table1 =
getFBrefStats("https://fbref.com/en/comps/22/Major-League-Soccer-Stats","#stats_keeper_squads")
head(table_name)
print(target)
write.csv(target, "D:\\Desktop\\Table1.csv")
write.csv(Table1, "D:\\Desktop\\Table1.csv")
table_name = "#stats_keeper_squads"
Table1 =
getFBrefStats("https://fbref.com/en/comps/22/Major-League-Soccer-Stats","#stats_keeper_squads")
head(table_name)
print(target)
write.csv(Table1, "D:\\Desktop\\Table1.csv")
print(target)
print(Table1)
Table1 =
getFBrefStats("https://fbref.com/en/comps/22/Major-League-Soccer-Stats","#stats_keeper_squads")
head(Table1)
write.csv(Table1, "D:\\Desktop\\Table1.csv")
Table1 =
getFBrefStats("https://fbref.com/en/comps/22/Major-League-Soccer-Stats","#stats_keeper_squads")
head(Table1)
write.csv(Table1, "D:\\Desktop\\Programming\\Football Statistics\\Veri\\SuperLig\\Table1.csv")
Table1 =
getFBrefStats("https://fbref.com/en/comps/22/keepersadv/Major-League-Soccer-Stats","#stats_keeper_adv")
head(Table1)
write.csv(Table1, "D:\\Desktop\\Programming\\Football Statistics\\Veri\\SuperLig\\Table1.csv")
Table1 =
getFBrefStats("https://fbref.com/en/comps/26/keepers/Super-Lig-Stats","#stats_keeper")
head(Table1)
write.csv(Table1, "D:\\Desktop\\Programming\\Football Statistics\\Veri\\SuperLig\\Table1.csv")
Table1 =
getFBrefStats("https://fbref.com/en/comps/26/playingtime/Super-Lig-Stats","#stats_playing_time")
head(Table1)
write.csv(Table1, "D:\\Desktop\\Programming\\Football Statistics\\Veri\\SuperLig\\Table1.csv")
Table1 =
getFBrefStats("https://fbref.com/en/comps/26/playingtime/Super-Lig-Stats","#stats_playing_time")
head(Table1)
write.csv(Table1, "D:\\Desktop\\Programming\\Football Statistics\\Veri\\SuperLig\\Table1.csv")
getFBrefStats = function(url,id){
require(RSelenium)
require(dplyr)
# For some unspecified reason we are starting and stopping the docker container initailly.
# Similar to heating the bike's engine before shifting the gears.
system("docker run -d -p 4445:4444 selenium/standalone-chrome")
t = system("docker ps",intern=TRUE)
if(is.na(as.character(strsplit(t[2],split = " ")[[1]][1]))==FALSE)
{
system(paste("docker stop ",as.character(strsplit(t[2],split = " ")[[1]][1]),sep=""))
}
# To avoid starting docker in Terminal
system("docker run -d -p 4445:4444 selenium/standalone-chrome")
Sys.sleep(3)
remDr <- RSelenium::remoteDriver(remoteServerAddr = "localhost", port = 4445L, browserName = "chrome")
# Automating the scraping initiation considering that Page navigation might crash sometimes in
# R Selenuium and we have to start the process again. Good to see that this while() logic
# works perfectly
while (TRUE) {
tryCatch({
#Entering our URL gets the browser to navigate to the page
remDr$open()
remDr$navigate(as.character(url))
}, error = function(e) {
remDr$close()
Sys.sleep(2)
print("slept 2 seconds")
next
}, finally = {
#remDr$screenshot(display = TRUE) #This will take a screenshot and display it in the RStudio viewer
break
})
}
# Scraping required stats
data <- xml2::read_html(remDr$getPageSource()[[1]]) %>%
rvest::html_nodes(id) %>%
rvest::html_table()
data = data[[1]]
remDr$close()
remove(remDr)
# Automating the following steps:
# 1. run "docker ps" in Terminal and get the container ID from the output
# 2. now run "docker stop container_id" e.g. docker stop f59930f56e38
t = system("docker ps",intern=TRUE)
system(paste("docker stop ",as.character(strsplit(t[2],split = " ")[[1]][1]),sep=""))
return(data)
}
# Target Website ----------------------------------------------------------
Table1 =
getFBrefStats("https://fbref.com/en/comps/26/playingtime/Super-Lig-Stats","#stats_playing_time")
head(Table1)
write.csv(Table1, "D:\\Desktop\\Programming\\Football Statistics\\Veri\\SuperLig\\Table1.csv")
comps
Table1 =
getFBrefStats("https://fbref.com/en/comps/26/playingtime/Super-Lig-Stats","#stats_shooting")
head(Table1)
write.csv(Table1, "D:\\Desktop\\Programming\\Football Statistics\\Veri\\SuperLig\\Table1.csv")
